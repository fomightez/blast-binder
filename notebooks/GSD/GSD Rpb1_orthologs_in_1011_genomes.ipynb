{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSD: Rpb1 orthologs in 1011 genomes collection\n",
    "\n",
    "This collects Rpb1 gene and protein sequences from a collection of natural isolates of sequenced yeast genomes from [Peter et al 2017](https://www.ncbi.nlm.nih.gov/pubmed/29643504), and then estimates the count of the heptad repeats. It builds directly on the notebook [here](GSD%20Rpb1_orthologs_in_PB_genomes.ipynb), which descends from [Searching for coding sequences in genomes using BLAST and Python](../Searching%20for%20coding%20sequences%20in%20genomes%20using%20BLAST%20and%20Python.ipynb). It also builds on the notebooks shown [here](https://nbviewer.jupyter.org/github/fomightez/cl_sq_demo-binder/blob/master/notebooks/GSD/GSD%20Add_Supplemental_data_info_to_nt_count%20data%20for%201011_cerevisiae_collection.ipynb) and [here](https://github.com/fomightez/patmatch-binder). \n",
    "\n",
    "Reference for sequence data:  \n",
    "[Genome evolution across 1,011 Saccharomyces cerevisiae isolates. Peter J, De Chiara M, Friedrich A, Yue JX, Pflieger D, BergstrÃ¶m A, Sigwalt A, Barre B, Freel K, Llored A, Cruaud C, Labadie K, Aury JM, Istace B, Lebrigand K, Barbry P, Engelen S, Lemainque A, Wincker P, Liti G, Schacherer J. Nature. 2018 Apr;556(7701):339-344. doi: 10.1038/s41586-018-0030-5. Epub 2018 Apr 11. PMID: 29643504](https://www.ncbi.nlm.nih.gov/pubmed/29643504)\n",
    "\n",
    "  \n",
    "  -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "![overview of steps](../../imgs/ortholog_mining_summarized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Get scripts and sequence data necessary.\n",
    "\n",
    "**DO NOT 'RUN ALL'. AN INTERACTION IS NECESSARY AT CELL FIVE. AFTER THAT INTERACTION, THE REST BELOW IT CAN BE RUN.**\n",
    "\n",
    "(Caveat: right now this is written for genes with no introns. Only a few hundred have in yeast and that is the organism in this example. Intron presence would only become important when trying to translate in late stages of this workflow.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_name = \"RPB1\"\n",
    "size_expected = 5202\n",
    "get_seq_from_link = False\n",
    "link_to_FASTA_of_gene = \"https://gist.githubusercontent.com/fomightez/f46b0624f1d8e3abb6ff908fc447e63b/raw/625eaba76bb54e16032f90c8812350441b753a0c/uz_S288C_YOR270C_VPH1_coding.fsa\"\n",
    "#**Possible future enhancement would be to add getting the FASTA of the gene from Yeastmine with just systematic id**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the `blast_to_df` script by running this commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_needed = \"blast_to_df.py\"\n",
    "if not os.path.isfile(file_needed):\n",
    "    !curl -O https://raw.githubusercontent.com/fomightez/sequencework/master/blast-utilities/blast_to_df.py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to get the entire collection or a subset of the 1011 genomes, the next cell will need to be edited.** I'll probably leave it with a small set for typical running purposes. However, to make it run fast, try the 'super-tiny' set with just two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get ALL the genomes. TAKES A WHILE!!! \n",
    "# (ca. 1 hour and 15 minutes to download alone? + Extracting is a while.)\n",
    "# Easiest way to minotor extracting step is to open terminal, cd to \n",
    "# `GENOMES_ASSEMBLED`, & use `ls | wc -l` to count files extracted.\n",
    "#!curl -O http://1002genomes.u-strasbg.fr/files/1011Assemblies.tar.gz\n",
    "#!tar xzf 1011Assemblies.tar.gz\n",
    "#!rm 1011Assemblies.tar.gz\n",
    "\n",
    "# Small development set\n",
    "small_set = True\n",
    "!curl -OL https://www.dropbox.com/s/f42tiygq9tr1545/medium_setGENOMES_ASSEMBLED.tar.gz\n",
    "!tar xzf medium_setGENOMES_ASSEMBLED.tar.gz\n",
    "\n",
    "# Tiny development set\n",
    "#!curl -OL https://www.dropbox.com/s/txufq2jflkgip82/tiny_setGENOMES_ASSEMBLED.tar.gz\n",
    "#!tar xzf tiny_setGENOMES_ASSEMBLED.tar.gz\n",
    "#!mv tiny_setGENOMES_ASSEMBLED GENOMES_ASSEMBLED\n",
    "\n",
    "#define directory with genomes\n",
    "genomes_dirn = \"GENOMES_ASSEMBLED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing the list of all of the assemblies, some data cleaning needs to be done. Specifically, we need to fix three that have an file name mismatches with contents in the description lines so this is consistent with the over 1000 others and for about a dozen we need to fix the description lines to match the conventions used in all others. The next two cells do these steps but **the cells must not be run until the data is unpacked**. (I tried to build in some checking to the first cell to alert the user.) *Note:* the tiny set doesn't contain any pertinent files so you can just skip these two cells for that set. While the small set also doesn't contain any pertinent files, I built in conditionals so they can be run for with either the small set or the entire collection of assemblies, as these are the sets most likely to be used. The tiny set is just for very simple debugging purposes.  \n",
    "The next cell address simple file name mismatch so that the file name matches the description line in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix names three files that don't match naming convention (small set has none of them)\n",
    "if not small_set:\n",
    "    import os\n",
    "    import sys\n",
    "    error2fix_dict = {\n",
    "        #\"CDH.re.fa\":\"CDH_3.re.fa\",\n",
    "        \"CFH.re.fa\":\"CFH_4.re.fa\",\n",
    "        \"CRL_.re.fa\":\"CRL_1.re.fa\"\n",
    "    }\n",
    "    for fn,fn_fix in error2fix_dict.items():\n",
    "        output_file_name = \"temp.txt\"\n",
    "        if os.path.isfile(\"GENOMES_ASSEMBLED/\"+fn):\n",
    "            sys.stderr.write(\"\\nFile with name non-matching entries ('{}') observed and\"\n",
    "                \" fixed.\".format(fn))\n",
    "            !mv GENOMES_ASSEMBLED/{fn} GENOMES_ASSEMBLED/{fn_fix}\n",
    "            #pause and then check if file with original name is there still because\n",
    "            # it means this was attempted too soon and need to start over.\n",
    "            import time\n",
    "            time.sleep(12) #12 seconds \n",
    "            if os.path.isfile(\"GENOMES_ASSEMBLED/\"+fn):\n",
    "                sys.stderr.write(\"\\n***PROBLEM. TRIED THIS CELL BEFORE FINISHED UPLOADING.\\n\"\n",
    "                    \"DELETE FILES ASSOCIATED AND START ALL OVER AGAIN WITH UPLOAD STEP***.\")\n",
    "        else:\n",
    "            sys.stderr.write(\"\\nFile '{}' not seen and so nothing done\"\n",
    "                \". Seems wrong.\".format(fn))\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell addresses changing the description line for about a dozen FASTA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text in description lines of about a dozen files so they follow \n",
    "# convention of other files (skipped automatically in case of small set because not pertinent)\n",
    "if not small_set:\n",
    "    import os\n",
    "    import sys\n",
    "    fn_and_text2remove_dict = {\n",
    "        \"BVH_1.re.fa\":\"_AC1LP.IND2\",\n",
    "        \"BEB_6.re.fa\":\"_C37T3ACXX.IND41b\",\n",
    "        \"CEN_4.re.fa\":\"_C3MA4ACXX.IND41b\",\n",
    "        \"CHE_4.re.fa\":\"_C3MC3ACXX.IND41b\",\n",
    "        \"CKT_5.re.fa\":\"_C4AK9ACXX.IND41b\",\n",
    "        \"BHS_1.re.fa\":\"_C399DACXX.IND41b\",\n",
    "        \"BTD_1.re.fa\":\"_AC1LP.IND45\",\n",
    "        \"CRL_.re.fa\":\"_AB3AC.IND41b\",\n",
    "        \"BTL_3.re.fa\":\"_C3G4CACXX.IND41b\",\n",
    "        \"CPB_4.re.fa\":\"_C4VAEACXX.IND41b\",\n",
    "        \"CAS_1.re.fa\":\"_AC1LP.IND6\",\n",
    "        \"BLH_1.re.fa\":\"_C37YTACXX.IND41b\",\n",
    "    }\n",
    "    output_file_name = \"temp.txt\"\n",
    "    for fn,text2remove in fn_and_text2remove_dict.items():\n",
    "        if os.path.isfile(\"GENOMES_ASSEMBLED/\"+fn):\n",
    "            # prepare output file for saving so it will be open and ready\n",
    "            with open(output_file_name, 'w') as output_file:\n",
    "\n",
    "                # read in the input file\n",
    "                with open(\"GENOMES_ASSEMBLED/\"+fn, 'r') as input_handler:\n",
    "                    for line in input_handler:\n",
    "                        if line.startswith(\">\"):\n",
    "                            new_line = line.replace(text2remove,\"\")\n",
    "                        else:\n",
    "                            new_line = line\n",
    "\n",
    "                        # Send text to output\n",
    "                        output_file.write(new_line)\n",
    "\n",
    "\n",
    "            # replace the original file with edited\n",
    "            !mv temp.txt GENOMES_ASSEMBLED/{fn}\n",
    "            sys.stderr.write(\"\\nDescription lines fixed to match final file name in '{}'\"\n",
    "                \" fixed.\".format(fn))\n",
    "        else:\n",
    "            sys.stderr.write(\"\\nFile '{}' not seen and so nothing done\"\n",
    "                \". Seems wrong.\".format(fn))\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SGD gene sequence in FASTA format to search for best matches in the genomes\n",
    "import sys\n",
    "gene_filen = gene_name + \".fsa\"\n",
    "if get_seq_from_link:\n",
    "    !curl -o {gene_filen} {link_to_FASTA_of_gene}\n",
    "else:\n",
    "    !touch {gene_filen}\n",
    "    sys.stderr.write(\"\\nEDIT THE FILE '{}' TO CONTAIN \"\n",
    "        \"YOUR GENE OF INTEREST (FASTA-FORMATTED)\"\n",
    "        \".\".format(gene_filen))\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I PUT CONTENTS OF FILE `S288C_YDL140C_RPO21_coding.fsa` downloaded from [here](https://www.yeastgenome.org/locus/S000002299/sequence) as 'RPB1.fsa'.**\n",
    "\n",
    "Now you are prepared to run BLAST to search each PacBio-sequenced genomes for the best match to a gene from the Saccharomyces cerevisiae strain S288C reference sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BLAST to search the genomes for matches to the gene in the reference genome at SGD\n",
    "\n",
    "SGD is the [Saccharomyces cerevisiae Genome Database site](http:yeastgenome.org) and the reference genome is from S288C.\n",
    "\n",
    "This is going to go through each genome and make a database so it is searchable and then search for matches to the gene. The information on the best match will be collected. One use for that information will be collecting the corresponding sequences later.\n",
    "\n",
    "Import the script that allows sending BLAST output to Python dataframes so that we can use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blast_to_df import blast_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all `genome.fa` files, excluding `genome.fa.nhr` and `genome.fa.nin` and `genome.fansq`\n",
    "# The excluding was only necessary because I had run some queries preliminarily in development. Normally, it would just be the `.re.fa` at the outset.\n",
    "fn_to_check = \"re.fa\" \n",
    "genomes = []\n",
    "import os\n",
    "import fnmatch\n",
    "for file in os.listdir(genomes_dirn):\n",
    "    if fnmatch.fnmatch(file, '*'+fn_to_check):\n",
    "        if not file.endswith(\".nhr\") and not file.endswith(\".nin\") and not file.endswith(\".nsq\") :\n",
    "            # plus skip hidden files\n",
    "            if not file.startswith(\"._\"):\n",
    "                genomes.append(file)\n",
    "len(genomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trick of putting `%%capture` on first line from [here](https://stackoverflow.com/a/23692951/8508004) to suppress the output from BLAST for many sequences from filling up cell.  \n",
    "(You can monitor the making of files ending in `.nhr` for all the FASTA files in `GENOMES_ASSEMBLED` to monitor progress'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "SGD_gene = gene_filen\n",
    "dfs = []\n",
    "for genome in genomes:\n",
    "    !makeblastdb -in {genomes_dirn}/{genome} -dbtype nucl\n",
    "    result = !blastn -query {SGD_gene} -db {genomes_dirn}/{genome} -outfmt \"6 qseqid sseqid stitle pident qcovs length mismatch gapopen qstart qend sstart send qframe sframe frames evalue bitscore qseq sseq\" -task blastn\n",
    "    from blast_to_df import blast_to_df\n",
    "    blast_df = blast_to_df(result.n)\n",
    "    dfs.append(blast_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes in the list `dfs` into one dataframe\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the df\n",
    "filen_prefix = gene_name + \"_orthologBLASTdf\"\n",
    "df.to_pickle(filen_prefix+\".pkl\")\n",
    "df.to_csv(filen_prefix+'.tsv', sep='\\t',index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally check if any genomes missing from the BLAST results list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjids = df.sseqid.tolist()\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "subjids = [x.split(\"-\")[0] for x in subjids]\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "len_genome_fn_end = len(fn_to_check) + 1 # plus one to accound for the period that will be \n",
    "# between `fn_to_check` and strain_id`, such as `SK1.genome.fa`\n",
    "genome_ids = [x[:-len_genome_fn_end] for x in genomes]\n",
    "#print (genome_ids[0:10])\n",
    "\n",
    "a = set(genome_ids)\n",
    "#print (a)\n",
    "print (\"initial:\",len(a))\n",
    "r = set(subjids)\n",
    "print(\"results:\",len(r))\n",
    "print (\"missing:\",len(a-r))\n",
    "if len(a-r):\n",
    "    print(\"\\n\")\n",
    "    print(\"ids missing:\",a-r)\n",
    "#a - r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: Report on how expected size compares to max size seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_seen = df.length.max(0)\n",
    "print (\"Expected size of gene:\", size_expected)\n",
    "print (\"Most frequent size of matches:\", df.length.mode()[0])\n",
    "print (\"Maximum size of matches:\", df.length.max(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the identified, raw sequences\n",
    "\n",
    "Get the expected size centered on the best match, plus a little flanking each because they might not exactly cover the entire open reading frame. (Although, the example here all look to be full size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the script for extracting based on position (and install dependency pyfaidx)\n",
    "import os\n",
    "file_needed = \"extract_subsequence_from_FASTA.py\"\n",
    "if not os.path.isfile(file_needed):\n",
    "    !curl -O https://raw.githubusercontent.com/fomightez/sequencework/master/Extract_from_FASTA/extract_subsequence_from_FASTA.py\n",
    "!pip install pyfaidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next cell, I am going to use the trick of putting `%%capture` on first line from [here](https://stackoverflow.com/a/23692951/8508004) to suppress the output from the entire set making a long list of output.\n",
    "For ease just monitor the progress in a launched terminal with the following code run in the directory where this notebook will be because the generated files only moved into the `raw` directory as last step of cell:\n",
    "\n",
    "    ls seq_extracted* | wc -l\n",
    "    \n",
    "(**NOTE: WHEN RUNNING WITH THE FULL SET, THIS CELL BELOW WILL REPORT AROUND A DOZEN `FileNotFoundError:`/Exceptions. HOWEVER, THEY DON'T CAUSE THE NOTEBOOK ITSELF TO CEASE TO RUN. SO DISREGARD THEM FOR THE TIME BEING.** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "size_expected = size_expected # use value from above, or alter at this point.\n",
    "#size_expected = df.length.max(0) #bp length of SGD coding sequence; should be equivalent and that way not hardcoded?\n",
    "extra_add_to_start = 51 #to allow for 'fuzziness' at starting end\n",
    "extra_add_to_end = 51 #to allow for 'fuzziness' at far end\n",
    "genome_fn_end = \"re.fa\" \n",
    "\n",
    "\n",
    "def midpoint(items):\n",
    "    '''\n",
    "    takes a iterable of items and returns the midpoint (integer) of the first \n",
    "    and second values\n",
    "    '''\n",
    "    return int((int(items[0])+int(items[1]))/2)\n",
    "\n",
    "#midpoint((1,100))\n",
    "\n",
    "def determine_pos_to_get(match_start,match_end):\n",
    "    '''\n",
    "    Take the start and end of the matched region.\n",
    "    \n",
    "    Calculate midpoint between those and then \n",
    "    center expected size on that to determine\n",
    "    preliminary start and preliminary end to get.\n",
    "    Add the extra basepairs to get at each end\n",
    "    to allow for fuzziness/differences of actual\n",
    "    gene ends for orthologs. \n",
    "    Return the final start and end positions to get.\n",
    "    \n",
    "    '''\n",
    "    center_of_match = midpoint((match_start,match_end))\n",
    "    half_size_expected = int(size_expected/2.0)\n",
    "    if size_expected % 2 != 0:\n",
    "        half_size_expected += 1\n",
    "    start_pos = center_of_match - half_size_expected\n",
    "    end_pos = center_of_match + half_size_expected\n",
    "    start_pos -= extra_add_to_start\n",
    "    end_pos += extra_add_to_end \n",
    "    \n",
    "    # Because of getting some flanking sequences to account for 'fuzziness', it \n",
    "    # is possible the start and end can exceed possible. 'End' is not a problem \n",
    "    # because the `extract_subsequence_from_FASTA.py` script will get as much as\n",
    "    # it from the indicated sequence if a larger than possible number is \n",
    "    # provided. However,'start' can become negative and because the region to \n",
    "    # extract is provided as a string the dash can become a problem. Dealing \n",
    "    # with it here by making sequence positive only.\n",
    "    # Additionally, because I rely on center of match to position where to get,\n",
    "    # part being cut-off due to absence on sequence fragment will shift center\n",
    "    # of match away from what is actually center of gene and to counter-balance\n",
    "    # add twice the amount to the other end. (Actually, I feel I should adjust\n",
    "    # the start end likewise if the sequence happens to be shorter than portion\n",
    "    # I would like to capture but I don't know length of involved hit yet and\n",
    "    # that would need to be added to allow that to happen!<--TO DO)\n",
    "    if start_pos < 0:\n",
    "        raw_amount_missing_at_start = abs(start_pos)# for counterbalancing; needs\n",
    "        # to be collected before `start_pos` adjusted\n",
    "        start_pos = 1\n",
    "        end_pos += 2 * raw_amount_missing_at_start\n",
    "        \n",
    "    return start_pos, end_pos\n",
    "\n",
    "\n",
    "\n",
    "# go through the dataframe using information on each to come up with sequence file, \n",
    "# specific indentifier within sequence file, and the start and end to extract\n",
    "# store these valaues as a list in a dictionary with the strain identifier as the key.\n",
    "extracted_info = {}\n",
    "start,end = 0,0\n",
    "for row in df.itertuples():\n",
    "    #print (row.length)\n",
    "    start_to_get, end_to_get = determine_pos_to_get(row.sstart, row.send)\n",
    "    posns_to_get = \"{}-{}\".format(start_to_get, end_to_get)\n",
    "    record_id = row.sseqid\n",
    "    strain_id = row.sseqid.split(\"-\")[0]\n",
    "    seq_fn = strain_id + \".\" + genome_fn_end\n",
    "    extracted_info[strain_id] = [seq_fn, record_id, posns_to_get]\n",
    "# Use the dictionary to get the sequences\n",
    "for id_ in extracted_info:\n",
    "    #%run extract_subsequence_from_FASTA.py {*extracted_info[id_]} #unpacking doesn't seem to work here in `%run`\n",
    "    %run extract_subsequence_from_FASTA.py {genomes_dirn}/{extracted_info[id_][0]} {extracted_info[id_][1]} {extracted_info[id_][2]}\n",
    "\n",
    "#package up the retrieved sequences\n",
    "archive_file_name = gene_name+\"_raw_ortholog_seqs.tar.gz\"\n",
    "# make list of extracted files using fnmatch\n",
    "fn_part_to_match = \"seq_extracted\"\n",
    "collected_seq_files_list = []\n",
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "for file in os.listdir('.'):\n",
    "    if fnmatch.fnmatch(file, fn_part_to_match+'*'):\n",
    "        #print (file)\n",
    "        collected_seq_files_list.append(file)\n",
    "!tar czf {archive_file_name} {\" \".join(collected_seq_files_list)} # use the list for archiving command\n",
    "sys.stderr.write(\"\\n\\nCollected RAW sequences gathered and saved as \"\n",
    "                 \"`{}`.\".format(archive_file_name))\n",
    "# move the collected raw sequences to a folder in preparation for\n",
    "# extracting encoding sequence from original source below\n",
    "!mkdir raw\n",
    "!mv seq_extracted*.fa raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That archive should contain the \"raw\" sequence for each gene, even if the ends are a little different for each. At minimum the entire gene sequence needs to be there at this point; extra at each end is preferable at this point.\n",
    "\n",
    "You should inspect them as soon as possible and adjust the extra sequence to add higher or lower depending on whether the ortholog genes vary more or less, respectively. The reason they don't need to be perfect yet though is because next we are going to extract the longest open reading frame, which presumably demarcates the entire gene. Then we can return to use that information to clean up the collected sequences to just be the coding sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect protein translations of the genes and then clean up \"raw\" sequences to just be coding\n",
    "\n",
    "We'll assume the longest translatable frame in the collected \"raw\" sequences encodes the protein sequence for the gene orthologs of interest. Well base these steps on the [section '20.1.13  Identifying open reading frames'](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc299) in the present version of the [Biopython Tutorial and Cookbook](http://biopython.org/DIST/docs/tutorial/Tutorial.html) (Last Update â 18 December 2018 (Biopython 1.73)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(First run the next cell to get a script needed for dealing with the strand during the translation and gathering of thge encoding sequence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_needed = \"convert_fasta_to_reverse_complement.py\"\n",
    "if not os.path.isfile(file_needed):\n",
    "    !curl -O https://raw.githubusercontent.com/fomightez/sequencework/master/ConvertSeq/convert_fasta_to_reverse_complement.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to perform the work described in the header to this section...\n",
    "\n",
    "For the next cell, I am going to use the trick of putting `%%capture` on first line from [here](https://stackoverflow.com/a/23692951/8508004) to suppress the output from the entire set making a long list of output.\n",
    "For ease just monitor the progress in a launched terminal with the following code run in the directory where this notebook will be:\n",
    "\n",
    "    ls *_ortholog_gene.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# find the featured open reading frame and collect presumed protein sequences\n",
    "# Collect the corresponding encoding sequence from the original source\n",
    "def len_ORF(items):\n",
    "    # orf is fourth item in the tuples\n",
    "    return len(items[3])\n",
    "def find_orfs_with_trans(seq, trans_table, min_protein_length):\n",
    "    '''\n",
    "    adapted from the present section '20.1.13  Identifying open reading frames'\n",
    "    http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc299 in the \n",
    "    present version of the [Biopython Tutorial and Cookbook at \n",
    "    http://biopython.org/DIST/docs/tutorial/Tutorial.html \n",
    "    (Last Update â 18 December 2018 (Biopython 1.73)\n",
    "    Same as there except altered to sort on the length of the\n",
    "    open reading frame.\n",
    "    '''\n",
    "    answer = []\n",
    "    seq_len = len(seq)\n",
    "    for strand, nuc in [(+1, seq), (-1, seq.reverse_complement())]:\n",
    "        for frame in range(3):\n",
    "            trans = str(nuc[frame:].translate(trans_table))\n",
    "            trans_len = len(trans)\n",
    "            aa_start = 0\n",
    "            aa_end = 0\n",
    "            while aa_start < trans_len:\n",
    "                aa_end = trans.find(\"*\", aa_start)\n",
    "                if aa_end == -1:\n",
    "                    aa_end = trans_len\n",
    "                if aa_end-aa_start >= min_protein_length:\n",
    "                    if strand == 1:\n",
    "                        start = frame+aa_start*3\n",
    "                        end = min(seq_len,frame+aa_end*3+3)\n",
    "                    else:\n",
    "                        start = seq_len-frame-aa_end*3-3\n",
    "                        end = seq_len-frame-aa_start*3\n",
    "                    answer.append((start, end, strand,\n",
    "                                   trans[aa_start:aa_end]))\n",
    "                aa_start = aa_end+1\n",
    "    answer.sort(key=len_ORF, reverse = True)\n",
    "    return answer\n",
    "\n",
    "def generate_rcoutput_file_name(file_name,suffix_for_saving = \"_rc\"):\n",
    "    '''\n",
    "    from https://github.com/fomightez/sequencework/blob/master/ConvertSeq/convert_fasta_to_reverse_complement.py\n",
    "    Takes a file name as an argument and returns string for the name of the\n",
    "    output file. The generated name is based on the original file\n",
    "    name.\n",
    "    Specific example\n",
    "    =================\n",
    "    Calling function with\n",
    "        (\"sequence.fa\", \"_rc\")\n",
    "    returns\n",
    "        \"sequence_rc.fa\"\n",
    "    '''\n",
    "    main_part_of_name, file_extension = os.path.splitext(\n",
    "        file_name) #from \n",
    "    #http://stackoverflow.com/questions/541390/extracting-extension-from-filename-in-python\n",
    "    if '.' in file_name:  #I don't know if this is needed with the os.path.splitext method but I had it before so left it\n",
    "        return main_part_of_name + suffix_for_saving  + file_extension\n",
    "    else:\n",
    "        return file_name + suffix_for_saving + \".fa\"\n",
    "    \n",
    "def add_strand_to_description_line(file,strand=\"-1\"):\n",
    "    '''\n",
    "    Takes a file and edits description line to add \n",
    "    strand info at end.\n",
    "    \n",
    "    Saves the fixed file\n",
    "    '''\n",
    "    import sys\n",
    "    output_file_name = \"temp.txt\"\n",
    "    # prepare output file for saving so it will be open and ready\n",
    "    with open(output_file_name, 'w') as output_file:\n",
    "\n",
    "        # read in the input file\n",
    "        with open(file, 'r') as input_handler:\n",
    "            # prepare to give feeback later or allow skipping to certain start\n",
    "            lines_processed = 0\n",
    "\n",
    "            for line in input_handler:\n",
    "                lines_processed += 1\n",
    "                if line.startswith(\">\"):\n",
    "                    new_line = line.strip() + \"; {} strand\\n\".format(strand)\n",
    "                else:\n",
    "                    new_line = line\n",
    "                \n",
    "                # Send text to output\n",
    "                output_file.write(new_line)\n",
    "\n",
    "    \n",
    "    # replace the original file with edited\n",
    "    !mv temp.txt {file}\n",
    "    # Feedback\n",
    "    sys.stderr.write(\"\\nIn {}, strand noted.\".format(file))\n",
    "\n",
    "table = 1 #sets translation table to standard nuclear, see \n",
    "# https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi\n",
    "min_pro_len = 80 #cookbook had the standard `100`. Feel free to adjust.\n",
    "prot_seqs_info = {} #collect as dictionary with strain_id as key. Values to\n",
    "# be list with source id as first item and protein length as second and \n",
    "# strand in source seq as third item, and start and end in source sequence as fourth and fifth,\n",
    "# and file name of protein and gene as sixth and seventh.\n",
    "# Example key and value pair: 'YPS138':['<source id>','<protein length>',-1,52,2626,'<gene file name>','<protein file name>']\n",
    "gene_seqs_fn_list = []\n",
    "prot_seqs_fn_list = []\n",
    "from Bio import SeqIO\n",
    "for raw_seq_filen in collected_seq_files_list:\n",
    "    #strain_id = raw_seq_filen[:-len_genome_fn_end] #if was dealing with source seq\n",
    "    strain_id = raw_seq_filen.split(\"-\")[0].split(\"seq_extracted\")[1]\n",
    "    record = SeqIO.read(\"raw/\"+raw_seq_filen,\"fasta\")\n",
    "    raw_seq_source_fn = strain_id + \".\" + genome_fn_end\n",
    "    raw_seq_source_id = record.description.split(\":\")[0]\n",
    "    orf_list = find_orfs_with_trans(record.seq, table, min_pro_len)\n",
    "    orf_start, orf_end, strand, prot_seq = orf_list[0] #longest ORF seq for protein coding\n",
    "    \n",
    "    location_raw_seq = record.description.rsplit(\":\",1)[1] #get to use in calculating\n",
    "    # the start and end position in original genome sequence.\n",
    "    raw_loc_parts = location_raw_seq.split(\"-\")\n",
    "    start_from_raw_seq = int(raw_loc_parts[0])\n",
    "    end_from_raw_seq = int(raw_loc_parts[1])\n",
    "    length_extracted = len(record) #also to use in calculating relative original\n",
    "    \n",
    "    #Fix negative value. (Somehow Biopython can report negative value when hitting\n",
    "    # end of sequence without encountering stop codon and negatives messes up \n",
    "    # indexing later it seems.)\n",
    "    if orf_start < 0:\n",
    "        orf_start = 0\n",
    "    \n",
    "    # Trim back to the first Methionine, assumed to be the initiating MET.\n",
    "    # (THIS MIGHT BE A SOURCE OF EXTRA 'LEADING' RESIDUES IN SOME CASES & ARGUES \n",
    "    # FOR LIMITING THE AMOUNT OF FLANKING SEQUENCE ADDED TO ALLOW FOR FUZINESS.)\n",
    "    try:\n",
    "        amt_resi_to_trim = prot_seq.index(\"M\")\n",
    "    except ValueError:\n",
    "        sys.stderr.write(\"**ERROR**When searching for initiating methionine,\\n\"\n",
    "                         \"no Methionine found in the traslated protein sequence.**ERROR**\")\n",
    "        sys.exit(1)\n",
    "    prot_seq = prot_seq[amt_resi_to_trim:]\n",
    "    len_seq_trimmed = amt_resi_to_trim * 3\n",
    "    \n",
    "    # Calculate the adjusted start and end values for the untrimmed ORF\n",
    "    adj_start = start_from_raw_seq + orf_start\n",
    "    adj_end = end_from_raw_seq - (length_extracted - orf_end)\n",
    "    \n",
    "    # Adjust for trimming for appropriate strand.\n",
    "    if strand == 1:\n",
    "        adj_start += len_seq_trimmed\n",
    "        #adj_end += 3 # turns out stop codon is part of numbering biopython returns\n",
    "    elif strand == -1:\n",
    "        adj_end -= len_seq_trimmed\n",
    "        #adj_start -= 3 # turns out stop codon is part of numbering biopython returns\n",
    "    else:\n",
    "        sys.stderr.write(\"**ERROR**No strand match option detected!**ERROR**\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Collect the sequence for the actual gene encoding region from\n",
    "    # the original sequence. This way the original numbers will\n",
    "    # be put in the file.\n",
    "    start_n_end_str = \"{}-{}\".format(adj_start,adj_end)\n",
    "    %run extract_subsequence_from_FASTA.py {genomes_dirn}/{raw_seq_source_fn} {raw_seq_source_id} {start_n_end_str}\n",
    "    \n",
    "    # rename the extracted subsequence a more distinguishing name and notify\n",
    "    g_output_file_name = strain_id +\"_\" + gene_name + \"_ortholog_gene.fa\"\n",
    "    !mv {raw_seq_filen} {g_output_file_name} # because the sequence saved happens to \n",
    "    # be same as raw sequence file saved previously, that name can be used to\n",
    "    # rename new file.\n",
    "    gene_seqs_fn_list.append(g_output_file_name)\n",
    "    sys.stderr.write(\"\\n\\nRenamed gene file to \"\n",
    "                     \"`{}`.\".format(g_output_file_name))\n",
    "    \n",
    "    # Convert extracted sequence to reverse complement if translation was on negative strand.\n",
    "    if strand == -1:\n",
    "        %run convert_fasta_to_reverse_complement.py {g_output_file_name}\n",
    "        # replace original sequence file with the produced file\n",
    "        produced_fn = generate_rcoutput_file_name(g_output_file_name)\n",
    "        !mv {produced_fn} {g_output_file_name}\n",
    "        # add (after saved) onto the end of the description line for that `-1 strand` \n",
    "        # No way to do this in my current version of convert sequence. So editing descr line.\n",
    "        add_strand_to_description_line(g_output_file_name)\n",
    "\n",
    "    \n",
    "    #When settled on actual protein encoding sequence, fill out\n",
    "    # description to use for saving the protein sequence.\n",
    "    prot_descr =  (record.description.rsplit(\":\",1)[0]+ \" \"+ gene_name \n",
    "                   + \"_ortholog\"+ \"| \" +str(len(prot_seq)) + \" aas | from \" \n",
    "                   + raw_seq_source_id + \" \"\n",
    "                   + str(adj_start) + \"-\"+str(adj_end))\n",
    "    if strand == -1:\n",
    "        prot_descr += \"; {} strand\".format(strand)\n",
    "    \n",
    "    # save the protein sequence as FASTA\n",
    "    chunk_size = 70 #<---amino acids per line to have in FASTA\n",
    "    prot_seq_chunks = [prot_seq[i:i+chunk_size] for i in range(\n",
    "        0, len(prot_seq),chunk_size)]\n",
    "    prot_seq_fa = \">\" + prot_descr + \"\\n\"+ \"\\n\".join(prot_seq_chunks)\n",
    "    p_output_file_name = strain_id +\"_\" + gene_name + \"_protein_ortholog.fa\"\n",
    "    with open(p_output_file_name, 'w') as output:\n",
    "        output.write(prot_seq_fa)\n",
    "    prot_seqs_fn_list.append(p_output_file_name)\n",
    "    sys.stderr.write(\"\\n\\nProtein sequence saved as \"\n",
    "                     \"`{}`.\".format(p_output_file_name))\n",
    "    \n",
    "    \n",
    "    # at end store information in `prot_seqs_info` for later making a dataframe \n",
    "    # and then text table for saving summary\n",
    "    #'YPS138':['<source id>',<protein length>,-1,52,2626,'<gene file name>','<protein file name>']\n",
    "    prot_seqs_info[strain_id] = [raw_seq_source_id,len(prot_seq),strand,adj_start,adj_end,\n",
    "                                 g_output_file_name,p_output_file_name]\n",
    "    \n",
    "    sys.stderr.write(\"\\n******END OF A SET OF PROTEIN ORTHOLOG \"\n",
    "                     \"AND ENCODING GENE********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use `prot_seqs_info` for saving a summary text table (first convert to dataframe?)\n",
    "table_fn_prefix = gene_name + \"_orthologs_table\"\n",
    "table_fn = table_fn_prefix + \".tsv\"\n",
    "pkl_table_fn = table_fn_prefix + \".pkl\"\n",
    "import pandas as pd\n",
    "info_df = pd.DataFrame.from_dict(prot_seqs_info, orient='index',\n",
    "    columns=['descr_id', 'length', 'strand', 'start','end','gene_file','prot_file']) # based on\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.from_dict.html and\n",
    "# note from Python 3.6 that `pd.DataFrame.from_items` is deprecated; \n",
    "#\"Please use DataFrame.from_dict\"\n",
    "info_df.to_pickle(pkl_table_fn)\n",
    "info_df.to_csv(table_fn, sep='\\t') # keep index is default\n",
    "sys.stderr.write(\"Text file of associated details saved as '{}'.\".format(table_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack up archive of gene and protein sequences plus the table\n",
    "seqs_list = gene_seqs_fn_list + prot_seqs_fn_list + [table_fn,pkl_table_fn]\n",
    "archive_file_name = gene_name+\"_ortholog_seqs.tar.gz\"\n",
    "!tar czf {archive_file_name} {\" \".join(seqs_list)} # use the list for archiving command\n",
    "sys.stderr.write(\"\\nCollected gene and protein sequences\"\n",
    "                 \" (plus table of details) gathered and saved as \"\n",
    "                 \"`{}`.\".format(archive_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tarballed archive to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the count of the heptad repeats\n",
    "\n",
    "Make a table of the estimate of heptad repeats for each orthlogous protein sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 'patmatch results to dataframe' script\n",
    "!curl -O https://raw.githubusercontent.com/fomightez/sequencework/master/patmatch-utilities/patmatch_results_to_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trick of putting `%%capture` on first line from [here](https://stackoverflow.com/a/23692951/8508004) to suppress the output from `patmatch_results_to_df` function from filling up cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "# Go through each protein sequence file and look for matches to heptad pattern\n",
    "\n",
    "# LATER POSSIBLE IMPROVEMENT. Translate pasted gene sequence and add SGD REF S228C as first in list `prot_seqs_fn_list`. Because \n",
    "# although this set of orthologs includes essentially S228C, other lists won't and best to have reference for comparing.\n",
    "\n",
    "\n",
    "heptad_pattern = \"[YF]SP[TG]SP[STAGN]\" # will catch repeats#2 through #26 of S288C according to Corden, 2013 PMID: 24040939\n",
    "\n",
    "from patmatch_results_to_df import patmatch_results_to_df\n",
    "sum_dfs = []\n",
    "raw_dfs = []\n",
    "for prot_seq_fn in prot_seqs_fn_list:\n",
    "    !perl ../../patmatch_1.2/unjustify_fasta.pl {prot_seq_fn}\n",
    "    output = !perl ../../patmatch_1.2/patmatch.pl -p {heptad_pattern} {prot_seq_fn}.prepared\n",
    "    os.remove(os.path.join(prot_seq_fn+\".prepared\")) #delete file made for PatMatch\n",
    "    raw_pm_df = patmatch_results_to_df(output.n, pattern=heptad_pattern, name=\"CTD_heptad\")\n",
    "    raw_pm_df.sort_values('hit_number', ascending=False, inplace=True)\n",
    "    sum_dfs.append(raw_pm_df.groupby('FASTA_id').head(1))\n",
    "    raw_dfs.append(raw_pm_df)\n",
    "sum_pm_df = pd.concat(sum_dfs, ignore_index=True)\n",
    "sum_pm_df.sort_values('hit_number', ascending=False, inplace=True)\n",
    "sum_pm_df = sum_pm_df[['FASTA_id','hit_number']]\n",
    "#make protein length into dictionary with ids as keys to map to FASTA_ids in \n",
    "# order to add protein length as a column in summary table\n",
    "length_info_by_id= dict(zip(info_df.descr_id,info_df.length))\n",
    "sum_pm_df['prot_length'] = sum_pm_df['FASTA_id'].map(length_info_by_id)\n",
    "sum_pm_df = sum_pm_df.reset_index(drop=True)\n",
    "raw_pm_df = pd.concat(raw_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of use of `%%capture` to suppress output, need a separate cell to see results summary. (Only showing parts here because will add more useful information below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pm_df.head() # don't show all yet since lots and want to make this dataframe more useful below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pm_df.tail() # don't show all yet since lots and want to make this dataframe more useful below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT ONES MISSING NOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally check if any genomes missing from the list of orthologs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjids = df.sseqid.tolist()\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "subjids = [x.split(\"-\")[0] for x in subjids]\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "len_genome_fn_end = len(fn_to_check) + 1 # plus one to accound for the period that will be \n",
    "# between `fn_to_check` and strain_id`, such as `SK1.genome.fa`\n",
    "genome_ids = [x[:-len_genome_fn_end] for x in genomes]\n",
    "#print (genome_ids[0:10])\n",
    "\n",
    "\n",
    "ortholg_ids = sum_pm_df.FASTA_id.tolist()\n",
    "ortholg_ids = [x.split(\"-\")[0] for x in ortholg_ids]\n",
    "\n",
    "a = set(genome_ids)\n",
    "#print (a)\n",
    "print (\"initial:\",len(a))\n",
    "r = set(subjids)\n",
    "print(\"BLAST results:\",len(r))\n",
    "print (\"missing from BLAST:\",len(a-r))\n",
    "if len(a-r):\n",
    "    #print(\"\\n\")\n",
    "    print(\"ids missing in BLAST results:\",a-r)\n",
    "#a - r\n",
    "\n",
    "print (\"\\n\\n=====POST-BLAST=======\\n\\n\")\n",
    "o = set(ortholg_ids)\n",
    "print(\"orthologs extracted:\",len(o))\n",
    "print (\"missing post-BLAST:\",len(r-o))\n",
    "if len(r-o):\n",
    "    print(\"\\n\")\n",
    "    print(\"ids lost post-BLAST:\",r-o)\n",
    "#r - o\n",
    "print (\"\\n\\n\\n=====SUMMARY=======\\n\\n\")\n",
    "if len(a-r) and len(r-o):\n",
    "    print(\"\\nAll missing in end:\",(a-r) | (r-o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Summarizing Dataframe more informative\n",
    "\n",
    "Add information on whether a stretch of 'N's is present. Making the data suspect and fit to be filtered out. Distinguish between cases where it is in what corresponds to the last third of the protein vs. elsewhere, if possible. Plus whether stop codon is present at end of encoding sequence because such cases also probably should be filtered out.\n",
    "\n",
    "Add information from the supplemental data table so possible patterns can be assessed more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add information about N stretches and stop codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect following information for each gene sequence: \n",
    "# N stretch of at least two or more present in first 2/3 of gene sequence\n",
    "# N stretch of at least two or more present in last 1/3 of gene sequence\n",
    "# stop codon encoded at end of sequence?\n",
    "\n",
    "import re\n",
    "min_number_Ns_in_row_to_collect = 2\n",
    "pattern_obj = re.compile(\"N{{{},}}\".format(min_number_Ns_in_row_to_collect), re.I)  # adpated from\n",
    "# code worked out in `collapse_large_unknown_blocks_in_DNA_sequence.py`, which relied heavily on\n",
    "# https://stackoverflow.com/a/250306/8508004\n",
    "def longest_stretch2ormore_found(string, pattern_obj):\n",
    "    '''\n",
    "    Check if a string has stretches of Ns of length two or more.\n",
    "    If it does, return the length of longest stretch.\n",
    "    If it doesn't return zero.\n",
    "    Based on https://stackoverflow.com/a/1155805/8508004 and\n",
    "    GSD Assessing_ambiguous_nts_in_nuclear_PB_genomes.ipynb\n",
    "    '''\n",
    "    longest_match = ''\n",
    "    for m in pattern_obj.finditer(string):\n",
    "        if len(m.group()) > len(longest_match):\n",
    "            longest_match = m.group()\n",
    "    if longest_match == '':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(longest_match)\n",
    "\n",
    "def chunk(xs, n):\n",
    "    '''Split the list, xs, into n chunks;\n",
    "    from http://wordaligned.org/articles/slicing-a-list-evenly-with-python'''\n",
    "    L = len(xs)\n",
    "    assert 0 < n <= L\n",
    "    s, r = divmod(L, n)\n",
    "    chunks = [xs[p:p+s] for p in range(0, L, s)]\n",
    "    chunks[n-1:] = [xs[-r-s:]]\n",
    "    return chunks\n",
    "    \n",
    "n_stretch_last_third_by_id = {}\n",
    "n_stretch_first_two_thirds_by_id = {}\n",
    "stop_codons = ['TAA','TAG','TGA']\n",
    "stop_codon_presence_by_id = {}\n",
    "for fn in gene_seqs_fn_list:\n",
    "    # read in sequence without using pyfaidx because small and not worth making indexing files\n",
    "    lines = []\n",
    "    with open(fn, 'r') as seqfile:\n",
    "        for line in seqfile:\n",
    "            lines.append(line.strip())\n",
    "    descr_line = lines[0]\n",
    "    seq = ''.join(lines[1:])\n",
    "    gene_seq_id = descr_line.split(\":\")[0].split(\">\")[1]#first line parsed for all in front of \":\" and without caret\n",
    "    \n",
    "    # determine first two-thirds and last third\n",
    "    chunks = chunk(seq,3)\n",
    "    assert len(chunks) == 3, (\"The sequence must be split in three parts'.\")\n",
    "    first_two_thirds = chunks[0] + chunks[1]\n",
    "    last_third = chunks[-1]\n",
    "    # Examine each part\n",
    "    n_stretch_last_third_by_id[gene_seq_id] = longest_stretch2ormore_found(last_third,pattern_obj)\n",
    "    n_stretch_first_two_thirds_by_id[gene_seq_id] = longest_stretch2ormore_found(first_two_thirds,pattern_obj)\n",
    "    #print(gene_seq_id)\n",
    "    #print (seq[-3:] in stop_codons)\n",
    "    #stop_codon_presence_by_id[gene_seq_id] = seq[-3:] in stop_codons\n",
    "    stop_codon_presence_by_id[gene_seq_id] = \"+\" if seq[-3:] in stop_codons else \"-\"\n",
    "\n",
    "# Add collected information to sum_pm_df\n",
    "sum_pm_df['NstretchLAST_THIRD'] = sum_pm_df['FASTA_id'].map(n_stretch_last_third_by_id)\n",
    "sum_pm_df['NstretchELSEWHERE'] = sum_pm_df['FASTA_id'].map(n_stretch_first_two_thirds_by_id)\n",
    "sum_pm_df['stop_codon'] = sum_pm_df['FASTA_id'].map(stop_codon_presence_by_id)\n",
    "# Safe to ignore any warnings about copy. I think because I swapped columns in and out\n",
    "# of sum_pm_df earlier perhaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add details on strains from the published supplemental information\n",
    "\n",
    "This section is based on [this notebook entitled 'GSD: Add Supplemental data info to nt count data for 1011 cerevisiae collection'](https://github.com/fomightez/cl_sq_demo-binder/blob/master/notebooks/GSD/GSD%20Add_Supplemental_data_info_to_nt_count%20data%20for%201011_cerevisiae_collection.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-018-0030-5/MediaObjects/41586_2018_30_MOESM3_ESM.xls\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#sum_pm_TEST_df = sum_pm_df.copy()\n",
    "supp_df = pd.read_excel('41586_2018_30_MOESM3_ESM.xls', sheet_name=0, header=3, skipfooter=31) \n",
    "supp_df['Standardized name'] = supp_df['Standardized name'].str.replace('SACE_','')\n",
    "suppl_info_dict = supp_df.set_index('Standardized name').to_dict('index')\n",
    "\n",
    "#Make new column with simplified strain_id tags to use for relating to supplemental table\n",
    "def add_id_tags(fasta_fn):\n",
    "    return fasta_fn[:3]\n",
    "sum_pm_df[\"id_tag\"] = sum_pm_df['FASTA_id'].apply(add_id_tags)\n",
    "\n",
    "ploidy_dict_by_id = {x:suppl_info_dict[x]['Ploidy'] for x in suppl_info_dict}\n",
    "aneuploidies_dict_by_id = {x:suppl_info_dict[x]['Aneuploidies'] for x in suppl_info_dict}\n",
    "eco_origin_dict_by_id = {x:suppl_info_dict[x]['Ecological origins'] for x in suppl_info_dict}\n",
    "clade_dict_by_id = {x:suppl_info_dict[x]['Clades'] for x in suppl_info_dict}\n",
    "sum_pm_df['Ploidy'] = sum_pm_df.id_tag.map(ploidy_dict_by_id) #Pandas docs has `Index.map` (uppercase `I`) but only lowercase works.\n",
    "sum_pm_df['Aneuploidies'] = sum_pm_df.id_tag.map(aneuploidies_dict_by_id)\n",
    "sum_pm_df['Ecological origin'] = sum_pm_df.id_tag.map(eco_origin_dict_by_id)\n",
    "sum_pm_df['Clade'] = sum_pm_df.id_tag.map(clade_dict_by_id)\n",
    "\n",
    "# remove the `id_tag` column add for relating details from supplemental to summary df\n",
    "sum_pm_df = sum_pm_df.drop('id_tag',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use following two lines when sure want to see all and COMMENT OUT BOTTOM LINE\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    display(sum_pm_df)\n",
    "sum_pm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter collected set to those that are 'complete'\n",
    "\n",
    "For plotting and summarizing with a good set of information, best to remove any where the identified ortholog gene has stretches of 'N's or lacks a stop codon.\n",
    "\n",
    "(Keep unfiltered dataframe around though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pm_UNFILTEREDdf = sum_pm_df.copy()\n",
    "#subset to those where there noth columns for Nstretch assessment are zero\n",
    "sum_pm_df = sum_pm_df[(sum_pm_df[['NstretchLAST_THIRD','NstretchELSEWHERE']] == 0).all(axis=1)] # based on https://codereview.stackexchange.com/a/185390\n",
    "#remove any where there isn't a stop codon\n",
    "sum_pm_df = sum_pm_df.drop(sum_pm_df[sum_pm_df.stop_codon != '+'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally summarize result of filtering in comparison to previous steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjids = df.sseqid.tolist()\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "subjids = [x.split(\"-\")[0] for x in subjids]\n",
    "#print (subjids)\n",
    "#print (subjids[0:10])\n",
    "len_genome_fn_end = len(fn_to_check) + 1 # plus one to accound for the period that will be \n",
    "# between `fn_to_check` and strain_id`, such as `SK1.genome.fa`\n",
    "genome_ids = [x[:-len_genome_fn_end] for x in genomes]\n",
    "#print (genome_ids[0:10])\n",
    "\n",
    "\n",
    "ortholg_ids = sum_pm_UNFILTEREDdf.FASTA_id.tolist()\n",
    "ortholg_ids = [x.split(\"-\")[0] for x in ortholg_ids]\n",
    "\n",
    "filtered_ids = sum_pm_df.FASTA_id.tolist()\n",
    "filtered_ids =[x.split(\"-\")[0] for x in filtered_ids]\n",
    "\n",
    "a = set(genome_ids)\n",
    "#print (a)\n",
    "print (\"initial:\",len(a))\n",
    "r = set(subjids)\n",
    "print(\"BLAST results:\",len(r))\n",
    "print (\"missing from BLAST:\",len(a-r))\n",
    "if len(a-r):\n",
    "    #print(\"\\n\")\n",
    "    print(\"ids missing in BLAST results:\",a-r)\n",
    "#a - r\n",
    "\n",
    "print (\"\\n\\n=====POST-BLAST=======\\n\\n\")\n",
    "o = set(ortholg_ids)\n",
    "print(\"orthologs extracted:\",len(o))\n",
    "print (\"missing post-BLAST:\",len(r-o))\n",
    "if len(r-o):\n",
    "    print(\"\\n\")\n",
    "    print(\"ids lost post-BLAST:\",r-o)\n",
    "#r - o\n",
    "print (\"\\n\\n\\n=====PRE-FILTERING=======\\n\\n\")\n",
    "print(\"\\nNumber before filtering:\",len(sum_pm_UNFILTEREDdf))\n",
    "if len(a-r) and len(r-o):\n",
    "    print(\"\\nAll missing in unfiltered:\",(a-r) | (r-o))\n",
    "print (\"\\n\\n\\n=====POST-FILTERING SUMMARY=======\\n\\n\")\n",
    "f = set(filtered_ids)\n",
    "print(\"\\nNumber left in filtered set:\",len(sum_pm_df))\n",
    "print (\"Number removed by filtering:\",len(o-f))\n",
    "if len(a-r) and len(r-o) and len(o-f):\n",
    "    print(\"\\nAll missing in filtered:\",(a-r) | (r-o) | (o-f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use following two lines when sure want to see all and COMMENT OUT BOTTOM LINE\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(sum_pm_df)\n",
    "#sum_pm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archive the 'Filtered' set of sequences\n",
    "\n",
    "Above I saved all the gene and deduced protein sequences of the orthologs in a single archive. It might be useful to just have an archive of the 'filtered' set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack up archive of gene and protein sequences for the 'filtered' set. \n",
    "# Include the summary table too.\n",
    "# This is different than the other sets I made because this 'filtering' was\n",
    "# done using the dataframe and so I don't have the file associations. The file names\n",
    "# though can be generated using the unfiltered file names for the genes and proteins\n",
    "# and sorting which ones don't remain in the filtered set using 3-letter tags at \n",
    "# the beginning of the entries in `FASTA_id` column to relate them.\n",
    "\n",
    "# Use the `FASTA_id` column of sum_pm_df to make a list of tags that remain in filtered set\n",
    "tags_remaining_in_filtered = [x[:3] for x in sum_pm_df.FASTA_id.tolist()]\n",
    "# Go through the gene and protein sequence list and collect those where the first\n",
    "# three letters match the tag\n",
    "gene_seqs_FILTfn_list = [x for x in gene_seqs_fn_list if x[:3] in tags_remaining_in_filtered]\n",
    "prot_seqs_FILTfn_list = [x for x in prot_seqs_fn_list  if x[:3] in tags_remaining_in_filtered]\n",
    "\n",
    "# Save the files in those two lists along with the sum_pm_df (as tabular data and pickled form)\n",
    "patmatchsum_fn_prefix = gene_name + \"_orthologs_patmatch_results_summary\"\n",
    "patmatchsum_fn = patmatchsum_fn_prefix + \".tsv\"\n",
    "pklsum_patmatch_fn = patmatchsum_fn_prefix + \".pkl\"\n",
    "import pandas as pd\n",
    "sum_pm_df.to_pickle(pklsum_patmatch_fn)\n",
    "sum_pm_df.to_csv(patmatchsum_fn, sep='\\t') # keep index is default\n",
    "FILTEREDseqs_n_df_list = gene_seqs_FILTfn_list + prot_seqs_FILTfn_list + [patmatchsum_fn,pklsum_patmatch_fn]\n",
    "archive_file_name = gene_name+\"_ortholog_seqsFILTERED.tar.gz\"\n",
    "!tar czf {archive_file_name} {\" \".join(FILTEREDseqs_n_df_list)} # use the list for archiving command\n",
    "sys.stderr.write(\"\\nCollected gene and protein sequences\"\n",
    "                 \" (plus table of details) for 'FILTERED' set gathered and saved as \"\n",
    "                 \"`{}`.\".format(archive_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the 'filtered' sequences to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing with filtered set\n",
    "\n",
    "Plot distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Want an image file of the figure saved?\n",
    "saveplot = True\n",
    "saveplot_fn_prefix = 'heptad_repeat_distribution'\n",
    "\n",
    "#sns.distplot(sum_pm_df[\"hit_number\"], kde=False, bins = max(sum_pm_df[\"hit_number\"]));\n",
    "p= sns.countplot(sum_pm_df[\"hit_number\"],\n",
    "    order = list(range(sum_pm_df.hit_number.min(),sum_pm_df.hit_number.max()+1)), \n",
    "    color=\"C0\", alpha= 0.93)\n",
    "    #palette=\"Blues\"); # `order` to get those categories with zero \n",
    "    # counts to show up from https://stackoverflow.com/a/45359713/8508004\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "\n",
    "#add percent above bars, based on code in middle of https://stackoverflow.com/a/33259038/8508004\n",
    "ncount = len(sum_pm_df)\n",
    "for pat in p.patches:\n",
    "    x=pat.get_bbox().get_points()[:,0]\n",
    "    y=pat.get_bbox().get_points()[1,1]\n",
    "    # note that this check on the next line was necessary to add when I went back to cases where there's\n",
    "    # no counts for certain categories and so `y` was coming up `nan` for for thos and causing error\n",
    "    # about needing positive value for the y value; `math.isnan(y)` based on https://stackoverflow.com/a/944733/8508004\n",
    "    if not math.isnan(y):\n",
    "        p.annotate('{:.1f}%'.format(100.*y/(ncount)), (x.mean(), y), ha='center', va='bottom', size = 9, color='#333333')\n",
    "\n",
    "\n",
    "if saveplot:\n",
    "    fig = p.get_figure() #based on https://stackoverflow.com/a/39482402/8508004\n",
    "    fig.savefig(saveplot_fn_prefix + '.png', bbox_inches='tight')\n",
    "    fig.savefig(saveplot_fn_prefix + '.svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with the entire 1011 collection, those at the bottom can not really be seen. The next plot shows this by limiting y-axis to 103.\n",
    "It should be possible to make a broken y-axis plot for this eventually but not right now as there is no automagic way. So for now will need to composite the two plots together outside.\n",
    "\n",
    "\n",
    "(Note that adding percents annotations makes height of this plot look odd in the notebook cell for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Want an image file of the figure saved?\n",
    "saveplot = True\n",
    "saveplot_fn_prefix = 'heptad_repeat_distributionLIMIT103'\n",
    "\n",
    "#sns.distplot(sum_pm_df[\"hit_number\"], kde=False, bins = max(sum_pm_df[\"hit_number\"]));\n",
    "p= sns.countplot(sum_pm_df[\"hit_number\"],\n",
    "    order = list(range(sum_pm_df.hit_number.min(),sum_pm_df.hit_number.max()+1)), \n",
    "    color=\"C0\", alpha= 0.93)\n",
    "    #palette=\"Blues\"); # `order` to get those categories with zero \n",
    "    # counts to show up from https://stackoverflow.com/a/45359713/8508004\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "plt.ylim(0, 103)\n",
    "\n",
    "#add percent above bars, based on code in middle of https://stackoverflow.com/a/33259038/8508004\n",
    "ncount = len(sum_pm_df)\n",
    "for pat in p.patches:\n",
    "    x=pat.get_bbox().get_points()[:,0]\n",
    "    y=pat.get_bbox().get_points()[1,1]\n",
    "    # note that this check on the next line was necessary to add when I went back to cases where there's\n",
    "    # no counts for certain categories and so `y` was coming up `nan` for those and causing error\n",
    "    # about needing positive value for the y value; `math.isnan(y)` based on https://stackoverflow.com/a/944733/8508004\n",
    "    if not math.isnan(y):\n",
    "        p.annotate('{:.1f}%'.format(100.*y/(ncount)), (x.mean(), y), ha='center', va='bottom', size = 9, color='#333333')\n",
    "\n",
    "if saveplot:\n",
    "    fig = p.get_figure() #based on https://stackoverflow.com/a/39482402/8508004\n",
    "    fig.savefig(saveplot_fn_prefix + '.png')\n",
    "    fig.savefig(saveplot_fn_prefix + '.svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# above line works for JupyterLab which I was developing in. Try  `%matplotlib notebook` for when in classic.\n",
    "\n",
    "# Visualization\n",
    "# This is loosely based on my past use of seaborn when making `plot_sites_position_across_chromosome.py` and related scripts.\n",
    "# For example, see `GC-clusters relative mito chromosome and feature` where I ran \n",
    "# `%run plot_sites_position_across_chromosome.py GC_df_for_merging.pkl -o strand_ofGCacross_mito_chrom`\n",
    "\n",
    "# add the strain info for listing that without chr info & add species information for coloring on that\n",
    "chromosome_id_prefix = \"-\"\n",
    "def FASTA_id_to_strain(FAid):\n",
    "    '''\n",
    "    use FASTA_id column value to convert to strain_id \n",
    "    and then return the strain_id\n",
    "    '''\n",
    "    return FAid.split(chromosome_id_prefix)[0]\n",
    "sum_pm_df_for_plot = sum_pm_df.copy()\n",
    "sum_pm_df_for_plot['strain'] = sum_pm_df['FASTA_id'].apply(FASTA_id_to_strain)\n",
    "# sum_pm_df['species'] = sum_pm_df['FASTA_id'].apply(strain_to_species) # since need species for label plot strips\n",
    "# it is easier to add species column first and then use map instead of doing both at same with one `apply`\n",
    "# of a function or both separately, both with `apply` of two different function.\n",
    "# sum_pm_df['species'] = sum_pm_df['strain'].apply(strain_to_species)\n",
    "sum_pm_df_for_plot['species'] = 'cerevisiae'\n",
    "\n",
    "#Want an image file of the figure saved?\n",
    "saveplot = True\n",
    "saveplot_fn_prefix = 'heptad_repeats_by_strain'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "if len(sum_pm_df) > 60:\n",
    "    plt.figure(figsize=(8,232))\n",
    "else:\n",
    "    plt.figure(figsize=(8,12))\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "# Simple look - Comment out everything below to the next two lines to see it again.\n",
    "p = sns.stripplot(x=\"hit_number\", y=\"strain\", data=sum_pm_df_for_plot, marker=\"h\", size=7.5, alpha=.98, palette=\"tab20b\")\n",
    "p = sns.stripplot(x=\"hit_number\", y=\"strain\", data=sum_pm_df_for_plot, marker=\"D\", size=9.5, alpha=.98, hue=\"Clade\")\n",
    "# NOTE CANNOT JUST USE ONE WITH `hue` by 'Clase' because several don't Clades assigned in the supplemental data\n",
    "# and so those left off. This overlays the two and doesn't cause artifacts when size of first maker smaller.\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "#p.set_xticklabels([\" \",\"23\",\" \",\"24\", \" \", \"25\"]) # This was much easier than all the stuff I tried for `Adjusted` look below\n",
    "# and the only complaint I have with the results is that what I assume are the `minor` tick lines show up; still ended up\n",
    "# needing this when added `xticks = p.xaxis.get_major_ticks()` in order to not show decimals for ones I kept\n",
    "#p.set(xticks=[]) # this works to remove the ticks entirely; however, I want to keep major ticks\n",
    "'''\n",
    "xticks = p.xaxis.get_major_ticks() #based on https://stackoverflow.com/q/50820043/8508004\n",
    "for i in range(len(xticks)):\n",
    "    #print (i) # WAS FOR DEBUGGING\n",
    "    keep_ticks = [1,3,5] #harcoding essentially again, but at least it works\n",
    "    if i not in keep_ticks:\n",
    "        xticks[i].set_visible(False)\n",
    "'''\n",
    "'''\n",
    "# Highly Adjusted look - Comment out default look parts above. Ended up going with simple above because still couldn't get\n",
    "# those with highest number of repeats with combination I could come up with.\n",
    "sum_pm_df_for_plot[\"repeats\"] = sum_pm_df_for_plot[\"hit_number\"].astype(str) # when not here (use `x=\"hit_number\"` in plot) or \n",
    "# tried `.astype('category')` get plotting of the 0.5 values too\n",
    "sum_pm_df_for_plot.sort_values('hit_number', ascending=True, inplace=True) #resorting again was necessary when\n",
    "# added `sum_pm_df[\"hit_number\"].astype(str)` to get 'lower' to 'higher' as left to right for x-axis; otherwise\n",
    "# it was putting the first rows on the left, which happened to be the 'higher' repeat values\n",
    "#p = sns.catplot(x=\"repeats\", y=\"strain\", hue=\"species\", data=sum_pm_df, marker=\"D\", size=10, alpha=.98) #marker size ignored in catplot?\n",
    "p = sns.stripplot(x=\"repeats\", y=\"strain\", hue=\"species\", data=sum_pm_df, marker=\"D\", size=10, alpha=.98)\n",
    "#p = sns.stripplot(x=\"repeats\", y=\"strain\", hue=\"species\", order = list(species_dict.keys()), data=sum_pm_df_for_plot, marker=\"D\", \n",
    "#    size=10, alpha=.98) # not fond of essentially harcoding to strain order but makes more logical sense to have\n",
    "    # strains with most repeats at the top of the y-axis; adding `order` makes `sort` order be ignored\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "sum_pm_df_for_plot.sort_values('hit_number', ascending=False, inplace=True) #revert to descending sort for storing df;\n",
    "'''\n",
    "if saveplot:\n",
    "    fig = p.get_figure() #based on https://stackoverflow.com/a/39482402/8508004\n",
    "    fig.savefig(saveplot_fn_prefix + '.png', bbox_inches='tight')\n",
    "    fig.savefig(saveplot_fn_prefix + '.svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Hexagons are used for those without an assigned clade in [the supplemental data Table 1](https://www.nature.com/articles/s41586-018-0030-5) in the plot above.)\n",
    "\n",
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# above line works for JupyterLab which I was developing in. Try  `%matplotlib notebook` for when in classic.\n",
    "\n",
    "# Visualization\n",
    "# This is loosely based on my past use of seaborn when making `plot_sites_position_across_chromosome.py` and related scripts.\n",
    "# For example, see `GC-clusters relative mito chromosome and feature` where I ran \n",
    "# `%run plot_sites_position_across_chromosome.py GC_df_for_merging.pkl -o strand_ofGCacross_mito_chrom`\n",
    "\n",
    "# add the strain info for listing that without chr info & add species information for coloring on that\n",
    "chromosome_id_prefix = \"-\"\n",
    "def FASTA_id_to_strain(FAid):\n",
    "    '''\n",
    "    use FASTA_id column value to convert to strain_id \n",
    "    and then return the strain_id\n",
    "    '''\n",
    "    return FAid.split(chromosome_id_prefix)[0]\n",
    "sum_pm_df_for_plot = sum_pm_df.copy()\n",
    "sum_pm_df_for_plot['strain'] = sum_pm_df['FASTA_id'].apply(FASTA_id_to_strain)\n",
    "# sum_pm_df['species'] = sum_pm_df['FASTA_id'].apply(strain_to_species) # since need species for label plot strips\n",
    "# it is easier to add species column first and then use map instead of doing both at same with one `apply`\n",
    "# of a function or both separately, both with `apply` of two different function.\n",
    "# sum_pm_df['species'] = sum_pm_df['strain'].apply(strain_to_species)\n",
    "sum_pm_df_for_plot['species'] = 'cerevisiae'\n",
    "\n",
    "#Want an image file of the figure saved?\n",
    "saveplot = True\n",
    "saveplot_fn_prefix = 'heptad_repeats_by_proteinlen'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "if len(sum_pm_df) > 60:\n",
    "    plt.figure(figsize=(8,232))\n",
    "else:\n",
    "    plt.figure(figsize=(8,12))\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "# Simple look - Comment out everything below to the next two lines to see it again.\n",
    "#p = sns.stripplot(x=\"hit_number\", y=\"strain\", data=sum_pm_df_for_plot, marker=\"h\", size=7.5, alpha=.98, palette=\"tab20b\")\n",
    "p = sns.stripplot(x=\"hit_number\", y=\"strain\", data=sum_pm_df_for_plot, marker=\"D\", size=9.5, alpha=.98, hue=\"prot_length\")\n",
    "# NOTE CANNOT JUST USE ONE WITH `hue` by 'Clase' because several don't Clades assigned in the supplemental data\n",
    "# and so those left off. This overlays the two and doesn't cause artifacts when size of first maker smaller.\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "#p.set_xticklabels([\" \",\"23\",\" \",\"24\", \" \", \"25\"]) # This was much easier than all the stuff I tried for `Adjusted` look below\n",
    "# and the only complaint I have with the results is that what I assume are the `minor` tick lines show up; still ended up\n",
    "# needing this when added `xticks = p.xaxis.get_major_ticks()` in order to not show decimals for ones I kept\n",
    "#p.set(xticks=[]) # this works to remove the ticks entirely; however, I want to keep major ticks\n",
    "'''\n",
    "xticks = p.xaxis.get_major_ticks() #based on https://stackoverflow.com/q/50820043/8508004\n",
    "for i in range(len(xticks)):\n",
    "    #print (i) # WAS FOR DEBUGGING\n",
    "    keep_ticks = [1,3,5] #harcoding essentially again, but at least it works\n",
    "    if i not in keep_ticks:\n",
    "        xticks[i].set_visible(False)\n",
    "'''\n",
    "'''\n",
    "# Highly Adjusted look - Comment out default look parts above. Ended up going with simple above because still couldn't get\n",
    "# those with highest number of repeats with combination I could come up with.\n",
    "sum_pm_df_for_plot[\"repeats\"] = sum_pm_df_for_plot[\"hit_number\"].astype(str) # when not here (use `x=\"hit_number\"` in plot) or \n",
    "# tried `.astype('category')` get plotting of the 0.5 values too\n",
    "sum_pm_df_for_plot.sort_values('hit_number', ascending=True, inplace=True) #resorting again was necessary when\n",
    "# added `sum_pm_df[\"hit_number\"].astype(str)` to get 'lower' to 'higher' as left to right for x-axis; otherwise\n",
    "# it was putting the first rows on the left, which happened to be the 'higher' repeat values\n",
    "#p = sns.catplot(x=\"repeats\", y=\"strain\", hue=\"species\", data=sum_pm_df, marker=\"D\", size=10, alpha=.98) #marker size ignored in catplot?\n",
    "p = sns.stripplot(x=\"repeats\", y=\"strain\", hue=\"species\", data=sum_pm_df, marker=\"D\", size=10, alpha=.98)\n",
    "#p = sns.stripplot(x=\"repeats\", y=\"strain\", hue=\"species\", order = list(species_dict.keys()), data=sum_pm_df_for_plot, marker=\"D\", \n",
    "#    size=10, alpha=.98) # not fond of essentially harcoding to strain order but makes more logical sense to have\n",
    "    # strains with most repeats at the top of the y-axis; adding `order` makes `sort` order be ignored\n",
    "p.set_xlabel(\"heptad repeats\")\n",
    "sum_pm_df_for_plot.sort_values('hit_number', ascending=False, inplace=True) #revert to descending sort for storing df;\n",
    "'''\n",
    "if saveplot:\n",
    "    fig = p.get_figure() #based on https://stackoverflow.com/a/39482402/8508004\n",
    "    fig.savefig(saveplot_fn_prefix + '.png', bbox_inches='tight')\n",
    "    fig.savefig(saveplot_fn_prefix + '.svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that '+ 2' should be added to the hit_number for each based on S288C according to [Corden, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24040939) (or `+1` like [Hsin and Manley, 2012](https://www.ncbi.nlm.nih.gov/pubmed/23028141)); however, that is something that could be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Make raw and summary data available for use elsewhere\n",
    "\n",
    "All the raw data is there for each strain in `raw_pm_df`. For example, the next cell shows how to view the data associated with the summary table for isolate ADK_8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADK_8_raw = raw_pm_df[raw_pm_df['FASTA_id'] == 'ADK_8-20587'].sort_values('hit_number', ascending=True).reset_index(drop=True)\n",
    "ADK_8_raw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary and raw data will be packaged up into one file in the cell below. One of the forms will be a tabular text data ('.tsv') files that can be opened in any spreadsheet software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary and raw results for use elsewhere (or use `.pkl` files for reloading the pickled dataframe into Python/pandas)\n",
    "patmatch_fn_prefix = gene_name + \"_orthologs_patmatch_results\"\n",
    "patmatchsum_fn_prefix = gene_name + \"_orthologs_patmatch_results_summary\"\n",
    "patmatchsumFILTERED_fn_prefix = gene_name + \"_orthologs_patmatch_results_summaryFILTERED\"\n",
    "patmatch_fn = patmatch_fn_prefix + \".tsv\"\n",
    "pkl_patmatch_fn = patmatch_fn_prefix + \".pkl\"\n",
    "patmatchsumUNF_fn = patmatchsumFILTERED_fn_prefix + \".tsv\"\n",
    "pklsum_patmatchUNF_fn = patmatchsumFILTERED_fn_prefix + \".pkl\"\n",
    "patmatchsum_fn = patmatchsum_fn_prefix + \".tsv\"\n",
    "pklsum_patmatch_fn = patmatchsum_fn_prefix + \".pkl\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "sum_pm_df.to_pickle(pklsum_patmatch_fn)\n",
    "sum_pm_df.to_csv(patmatchsum_fn, sep='\\t') # keep index is default\n",
    "sys.stderr.write(\"Text file of summary details after filtering saved as '{}'.\".format(patmatchsum_fn))\n",
    "sum_pm_UNFILTEREDdf.to_pickle(pklsum_patmatchUNF_fn)\n",
    "sum_pm_UNFILTEREDdf.to_csv(patmatchsumUNF_fn, sep='\\t') # keep index is default\n",
    "sys.stderr.write(\"\\nText file of summary details before filtering saved as '{}'.\".format(patmatchsumUNF_fn))\n",
    "raw_pm_df.to_pickle(pkl_patmatch_fn)\n",
    "raw_pm_df.to_csv(patmatch_fn, sep='\\t') # keep index is default\n",
    "sys.stderr.write(\"\\nText file of raw details saved as '{}'.\".format(patmatchsum_fn))\n",
    "# pack up archive dataframes\n",
    "pm_dfs_list = [patmatch_fn,pkl_patmatch_fn,patmatchsumUNF_fn,pklsum_patmatchUNF_fn, patmatchsum_fn,pklsum_patmatch_fn]\n",
    "archive_file_name = patmatch_fn_prefix+\".tar.gz\"\n",
    "!tar czf {archive_file_name} {\" \".join(pm_dfs_list)} # use the list for archiving command\n",
    "sys.stderr.write(\"\\nCollected pattern matching\"\n",
    "                 \" results gathered and saved as \"\n",
    "                 \"`{}`.\".format(archive_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the tarballed archive of the files to your computer.\n",
    "\n",
    "For now that archive doesn't include the figures generated from the plots because with a lot of strains they can get large. Download those if you want them. (Look for `saveplot_fn_prefix` settings in the code to help identify file names.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def executeSomething():\n",
    "    #code here\n",
    "    print ('.')\n",
    "    time.sleep(480) #60 seconds times 8 minutes\n",
    "\n",
    "while True:\n",
    "    executeSomething()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
